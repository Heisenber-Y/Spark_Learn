GraphBase测试文档


测试场景一:文件导入测试

案例一:批量导入：
(本次所有操作均在177服务器上执行)
操作指令：
1:进入批处理目录
cd  /tpdata/hadoopclient/GraphBase/graphwriter
2: 执行批量导入脚本(参数1为图数据库名称，参数2为数据文件在集群位置，参数3为映射文件，参数4为描述文		件)
     sh  ./bin/graphWriter.sh graphbase_1 /user/graphtest/graph_person_100w.txt /user/graphtest/Person.csv.mapper 	    	/user/graphtest/Person.desc 
数据量	文件名称	文件大小	资源消耗	执行时间
100w	graph_person_100w.txt	75M	21 81 420864	8min
500w	graph_person_500w.txt	413M	21 81 420864	24min
5000w	graph_person_5000w.txt	4.5G	21 81 420864	7h
10000w	graph_person_10kw.txt	9.0G	N/A	N/A
注释：资源消耗的三个值分别为 Running Contains, Allocated CPU VCores, Allocated Memory MB,
结果分析：
01 在相同资源消耗情况下，导入的数据量越大，导入的时间越长，并且导入数据量和导入时间不成正比关系。
02 当前版本的BulkLoad导入方式尚存在部分bug，导致使用该方式的导入时间比正常导入的时间成倍的增加，大数据量的文件消耗几个小时仍旧未能成功导入，此版本不推荐使用该方法。
03 在修改hbase参数：hbase.regionserver.hfile.durable.sync=false 和hbase.regionserver.wal.durable.sync=false后，导入速度明显提升，100万数据量5min左右，500万数据量9分钟，5000万数据量为1个小时，比实时导入的速度更快，但是此配置当前不可设置为session级，需要重启hbase组件才可还原设置，生产环境需要重新讨论后再决定是否启用该配置。

案例二：实时导入：
操作指令：
1:  进入实时导入目录
cd  /tpdata/hadoopclient/GraphBase/graphstream
2:  将需要导入的数据存入Kafka对应的topic中
./bin/graphProducer.sh
3: 通过SparkStreaming实时消费Kafka数据，然后将数据导入Hbase中(参数为对应的图数据库名称)
 	./bin/graphStreaming.sh graphbase_1

数据量	文件名称	文件大小	资源消耗	执行时间
100w	graph_person_100w.txt	75M	21 81 420864	3min
500w	graph_person_500w.txt	413M	21 81 420864	6min
5000w	graph_person_5000w.txt	4.5G	21 81 420864	1h50min
10000w	graph_person_10kw.txt	9.0G	21 81 412672	3 h 26 min
注释：资源消耗的三个值分别为 Running Contains, Allocated CPU VCores, Allocated Memory MB,
结果分析：
01 在相同资源消耗情况下，导入的数据量越大，导入的时间越长，导入数据量和导入时间不成正比关系
02 在相同资源消耗和相同数据量的情况下，实时导入的速度要比批量导入的速度快3-4倍左右


测试场景二:查询测试

案例一:通过Java api查询：
操作指令：
1: 点查询
String vertexId = getVertexIdByProperty(api,graphName,"person","name","marko");
api.queryVertex(vertexId, graphName);
2: 边查询
String edgeId = getEdgeIdByProperty(api,graphName,"call","weight","0.6");
api.queryEdge(edgeId, graphName);
3: 全图点查询
api.searchVertex(vertexSearchReqObj, graphName);
4: 全图边查询
api.searchEdge(edgeSearchReqObj, graphName);
5: 路径查询
allPathSearch(api, graphName);
6: 扩线查询
lineSearch(api, graphName);


查询详情	数据量	第一次查询时间（ms）	第二次查询时间（ms）	第三次查询时间（ms）	平均时间
（ms）
点查询	100万	285	309	353	315.67 
	500万	398	302	338	346.00 
	5000万	495	519	543	519.00 
边查询	100万	427	376	347	383.33 
	500万	449	532	459	480.00 
	5000万	758	429	305	497.33 
全图点查询	100万	649	532	411	530.67 
	500万	642	560	748	650.00 
	5000万	5459	3056	2923	3812.67 
全图边查询	100万	397	338	322	352.33 
	500万	1841	1954	1628	1807.67 
	5000万	2752	793	15645	6396.67 
路径查询	100万	431	486	498	471.67 
	500万	578	472	485	511.67 
	5000万	1008	574	631	737.67 
扩线查询	100万	230	427	235	297.33 
	500万	484	351	342	392.33 
	5000万	779	527	485	597.00 
结果分析：
01 通过Java Api查询数据，点查询,边查询,路径查询,扩线查询的速度较快，都是毫秒级，在500ms左右，但是在全图的点查询和全图的边查询时，因存在部分条件过滤的复杂度原因，所以此时的查询速度会变慢很多，并且和数据量也存在一定原因，查询速度平均在1~2秒之间
02 数据的查询速度和数据量也有一定关系，当数据量较大时，查询的速度也会随之变慢，500万的数据量比100万的数据量要慢10%-20%，5000万的数据量比500万数据量要慢30%左右。

案例二:通过Gremlin api查询：

查询详情	数据量	第一次查询时间（ms）	第二次查询时间（ms）	第三次查询时间（ms）	平均时间
（ms）
点查询	100万	1079	1403	1300	1260.67 
	500万	1326	2303	1106	1578.33 
	5000万	1347	1349	1284	1326.67 
边查询	100万	1132	1292	1128	1184.00 
	500万	1151	1240	1160	1183.67 
	5000万	1171	1215	1057	1147.67 
图对象遍历	100万	1240	1237	1149	1208.67 
	500万	1169	1314	1716	1399.67 
	5000万	1445	1216	1123	1261.33 
路径查询	100万	1098	1157	1202	1152.33 
	500万	1146	1148	1497	1263.67 
	5000万	1123	1341	1087	1183.67 
条件过滤：	100万	1353	1289	1486	1376.00 
	500万	2313	1410	1322	1681.67 
	5000万	1884	2733	5589	3402.00 
结果分析：
01  gremlin Api的查询速度要比Java Api的查询速度慢一倍左右
02 数据量的大小对查询速度的影响并不是很大，普遍都在1S左右
03 基础的点,边查询和复杂的图对象遍历,路径查询,条件过滤的查询速度对比没有明显时间差异
04 gremlin的查询速度虽然比Rest 查询速度慢很多，但是能够简洁的展现复杂的图形遍历和多步操作，在构建更大，更复杂的的查询时，这种构建遍历/查询的方式比rest Api 更方便简洁

测试场景三:资源大小和parallelism参数对实时导入速度的影响测试
(本次测试的数据量为100万)
案例一:设置cpu为80核，内存为400G，通过调整parallelism参数测试导入速度

资源配置	parallelism	调度延时
(平均)	处理时间
(平均)	总延时
(平均)	处理时间
80core 400G	10	1 min 31s 	948 ms	1 min 32s 	4 min 27 s
80core 400G	50	1 min 29s 	920 ms	1 min 30s 	4 min 24 s
80core 400G	100	1 min 33s	987 ms	1 min 34s 	3 min 48 s
80core 400G	150	1 min 41s 	944 ms	1 min 42s 	4 min 39 s 
80core 400G	200	1 min 27s 	984 ms	1 min 28s 	3 min 24 s
80core 400G	240	1 min 39s 	984 ms	1 min 40s 	4 min 
结果分析：
01  cpu为80核，内存为400G的时候，parallelism参数的大小对导入时间没有明显影响


案例二:设置cpu为80核，内存为200G，通过调整parallelism参数测试导入速度

资源配置	parallelism	调度延时
(平均)	处理时间
(平均)	总延时
(平均)	处理时间
80core 200G	10	1 min 42s 	935 ms	1 min 43s 	4 min 49 s
80core 200G	50	1 min 28s 	912 ms	1 min 29s 	4 min 27 s
80core 200G	100	1 min 16s	850 ms	1 min 16s 	4 min 32 s 
80core 200G	150	1 min 21s	962 ms	1 min 22s 	3 min 51 s
80core 200G	200	1 min 24s 	919 ms	1 min 25s 	4 min 21 s
80core 200G	240	1 min 24s 	976 ms	1 min 25s 	3 min 49 s
结果分析：
01  cpu为80核，内存为200G的时候，parallelism参数的大小对导入时间没有明显影响
02 在cpu核数相同的时候，内存为400G和200G对导入时间没有明显变化，基本都在3到4分钟左右时间导入完所有数据


案例三:设置cpu为80核，内存为100G，通过调整parallelism参数测试导入速度

资源配置	parallelism	调度延时
(平均)	处理时间
(平均)	总延时
(平均)	处理时间
80core 100G	10	1 min 18s 	858 ms	1 min 19s 	4 min 33 s
80core 100G	50	1 min 49s 	951 ms	1 min 50s 	4 min 59 s
80core 100G	100	1 min 43s 	918 ms	1 min 44s 	5 min 8 s
80core 100G	150	2 min 8s 	994 ms	2 min 9s 	4 min 55 s
80core 100G	200	2 min 7s 	973 ms	2 min 8s 	5 min 1 s
80core 100G	240	1 min 37s 	972 ms	1 min 38s 	4 min 14 s
结果分析：
01  在cpu核数相同的时候，当内存为100G的时候，导入时间要比内存为200G的速度稍慢，并且延时也要比200G内存的时候延时更高一点。
02  修改parallelism 参数对导入速度没有明显变化


案例四:设置cpu为40核，内存为200G，通过调整parallelism参数测试导入速度

资源配置	parallelism	调度延时
(平均)	处理时间
(平均)	总延时
(平均)	处理时间
40core 200G	10	2 mins 29s 	902 ms	2 min 30s	4 min 14 s
40core 200G	50	1 mins 33s 	984 ms	1 min 34s 	4 min 1 s
40core 200G	100	2 mins 34s 	985 ms	2 min 35s 	6 min 21 s
40core 200G	150	2 mins 10s 	993 ms	2 min 11s 	5 min 18 s
40core 200G	200	2 mins 43s 	967 ms	2 min 44s 	6 min 39s
40core 200G	240	2 mins 22s 	990 ms	2 min 23s 	5 min 50s
结果分析：
01 设置cpu核数为40核，内存为200G的时候，导入的处理时间和平均延时比80核，200G内存的速度要慢15%		左右  
02  修改parallelism 参数对导入速度也没有比较明显的变化，设置为10到50的时候速度要比100到200的速度要稍快

案例五:设置cpu为20核，内存为100G，通过调整parallelism参数测试导入速度

资源配置	parallelism	调度延时
(平均)	处理时间
(平均)	总延时
(平均)	处理时间
20core 100G	10	3 mins 8s 	995 ms	3 min 9s 	7 min 10 s
20core 100G	50	3 mins 20s 	969 ms	3 min 21s 	8 min 21 s
20core 100G	100	3 mins 16s 	998 ms	3 min 17s 	7 min 21 s
20core 100G	150	2 mins 35s 	993 ms	2 min 36s 	5 min 49 s
20core 100G	200	3 mins 28s 	994 ms	3 min 29s 	7 min 35 s
20core 100G	240	2 mins 24s	988 ms	2 min 25s	5 min 16 s
结果分析：
01 设置cpu核数为20核，内存为100G的时候，导入的处理时间和平均延时比40核，200G内存的速度要慢20%		左右 
02 修改parallelism 参数对导入速度也没有比较明显的变化
03  excutor的个数和每个excutor的core的个数的乘积，决定了任务并发能力 					      spark.streaming.partition.number		分区个数决定了总共启动任务的个数,在资源充足的情况下，可以将两      		者配成一致。
04  parallelism为导入数据的分区数，代表了个stage中task的数量，理想状况下为Executor number*Executor core的2-3倍即可，数据量较少的时候可以设置成80左右，当数据量达到千万级或者以上的时候，可以适当提高该参数，可以设置到150到200左右，具体情况视数据量来修改。



测试场景三:通过Java Api进行多线程导入并发测试：

案例一:数据插入测试
数据条数	线程数	消耗时间(秒)
100	10	9.4
100	50	4.9
100	100	5.0
100	250	4.9
100	500	9.4








数据条数	线程数	消耗时间(秒)
100	10	7.7
100	50	8.0
100	100	5.0
100	250	7.1
100	500	5.7
案例二:数据查询测试








数据条数	线程数	消耗时间(秒)
100	10	3.6
100	50	4.0
100	100	3.8
100	250	4.2
100	500	4.8
案例三:扩线查询查询测试







结果分析：通过设置不同线程数量来测试通过Java Api来进行导入数据,查询数据和扩线查询数据测试，由于当前通过Java插入的速度较慢，当插入10000条数据或者更多的数据量的收，消耗的时间，在14分钟左右，本次设置100数据量进行测试时，适当提高线程数对插入速度，查询速度有部分提升，但是当线程数增加到一定数量后，插入速度和查询速度并没有继续提升，当前版本通过Java Api插入数据的并发能力并不是很好，建议使用批量导入或者使用实时导入来进行图数据导入，后期1月份会有一个6.5.1版本的补丁，会在Java Api中添加批量导入的接口，用于提升Java Api的导入能力。


